{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXoC6Zi47cTXYfuqIx85Ub",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kn9ck/MAT422/blob/master/HW_3_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HW 3.7"
      ],
      "metadata": {
        "id": "aWahekFp22Gw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mathematical Formulation\n",
        "In a neural network, the goal is to transform inputs through a series of weights and biases to make predictions. Each layer in a neural network applies a linear transformation followed by an activation function to produce its output.\n",
        "\n",
        "The output of a node in a layer $l$ can be calculated by taking the weighted sum of the inputs from the previous layer, adding a bias, and applying an activation function.\n",
        "\n",
        "$$z^l = \\textbf{W}^l \\textbf{a}^{l-1} + \\textbf{b}^l$$\n",
        "where:\n",
        "* $\\textbf{W}^l$ is the weight matrix for layer $l$\n",
        "* $\\textbf{a}^{l-1}$ is the output of the previous layer\n",
        "* $\\textbf{b}^l$ is the bias vector\n",
        "* $z^l$ is the pre-activation output, which will be passed through an activation function.\n",
        "\n"
      ],
      "metadata": {
        "id": "BKQ0QXVj2tpj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdxlgXtDo0W5",
        "outputId": "40326029-7a90-49d3-e0d5-44fcefabad85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weighted Sum (z): 0.42000000000000004\n",
            "Outupt after activation: 0.6034832498647263\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "#define inputs and weights for a single layer neural network\n",
        "inputs = np.array([0.5, -0.2])  # x1 and x2\n",
        "weights = np.array([0.8, 0.4])  # w1 and w2\n",
        "bias = 0.1\n",
        "\n",
        "#output\n",
        "z = np.dot(weights, inputs) + bias\n",
        "print(\"weihgted sum (z):\", z)\n",
        "\n",
        "#apply activation function\n",
        "output = 1 / (1 + np.exp(-z))  #sigmoid function\n",
        "print(\"Outupt after activation:\", output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation Functions\n",
        "An **activation function** determines whether a neuron \"fires\" and thus contributes to the network's output. Different activation functions serve different purposes in neural networks.\n",
        "\n",
        "The activation function applies non-linearity, enabling the network to solve complex tasks.\n",
        "\n",
        "* **ReLU**: $\\sigma (x) = \\text{max}(0,x)$, allows faster and more effective training, especially in deeper networks.\n",
        "* **Sigmoid**: $\\sigma (x) = \\frac{1}{1+e^{-x}}$, maps the output between 0 and 1, useful in binary classification.\n",
        "\n",
        "* **Softmax**: used in classification, it outputs probabilities for each class."
      ],
      "metadata": {
        "id": "f8_nhXQ228th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ReLU\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "#sigmoid\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "#softmax\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))\n",
        "    return exp_x / exp_x.sum()\n",
        "\n",
        "z = np.array([-1, 0, 1])\n",
        "print(\"ReLU:\", relu(z))\n",
        "print(\"Sigmoid:\", sigmoid(z))\n",
        "print(\"Softmax:\", softmax(z))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6y9iEwe62_fX",
        "outputId": "9bd0817e-aee1-402c-8c91-ac58f32780d2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ReLU: [0 0 1]\n",
            "Sigmoid: [0.26894142 0.5        0.73105858]\n",
            "Softmax: [0.09003057 0.24472847 0.66524096]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cost Function\n",
        "The cost function measures how well the neural network's predictions match the true labels. It guides the learning process by quantifying the \"error\" between predictions and the actual outputs.\n",
        "\n",
        "* For regression tasks, a common cost function is **Mean Squared Error** (MSE): $$J = \\frac{1}{2} \\sum_{n=1}^{N} \\sum_{k=1}^{K}(\\hat{y}_k^{(n)}-{y}_k^{(n)})^2$$\n",
        "* For classification tasks, particularly binary classification, **cross-entropy loss** is often used: $$J=-\\sum_{n=1}^{N}(y^{(n)}\\ln \\hat{y}^{(n)}+(1-y^{(n)}) \\ln (1- \\hat{y}^{(n)}))$$"
      ],
      "metadata": {
        "id": "p_-V6M543BTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mean Squared Error\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Cross-Entropy\n",
        "def cross_entropy(y_true, y_pred):\n",
        "    return -np.sum(y_true * np.log(y_pred))\n",
        "\n",
        "# test with sample outputs\n",
        "y_true = np.array([1, 0, 0])\n",
        "y_pred = np.array([0.8, 0.1, 0.1])\n",
        "\n",
        "print(\"MSE:\", mse(y_true, y_pred))\n",
        "print(\"Cross-Entropy:\", cross_entropy(y_true, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmhz2Kw53Ciu",
        "outputId": "dc7c34a9-2985-473b-aeca-b8725db74569"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.019999999999999993\n",
            "Cross-Entropy: 0.2231435513142097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backpropagation\n",
        "**Backpropagation** is a process to compute the gradient of the cost function with respect to each weight, helping to minimize the cost function by adjusting the weights and biases.\n",
        "\n",
        "**Backpropagation** calculates how changing each weight and bias affects the cost function. It does this layer by layer from the output back to the input.\n",
        "\n",
        "For each layer, we calculate the \"error\" (delta) at each node, which depends on the derivative of the activation function and the error from the previous layer."
      ],
      "metadata": {
        "id": "DxEZN-W93ENq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_pass(weights, bias, inputs, output, target, learning_rate=0.01):\n",
        "    #gradient of cost function with respect to output\n",
        "    error = output - target\n",
        "\n",
        "    #gradient of output with respect to z (using the derivative of sigmoid)\n",
        "    d_output_d_z = output * (1 - output)\n",
        "\n",
        "    #gadient of cost with respect to z\n",
        "    d_cost_d_z = error * d_output_d_z\n",
        "\n",
        "    #update weights and bias\n",
        "    weights -= learning_rate * d_cost_d_z * inputs\n",
        "    bias -= learning_rate * d_cost_d_z\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "inputs = np.array([0.5, -0.2])\n",
        "weights = np.array([0.8, 0.4])\n",
        "bias = 0.1\n",
        "target = 1  # desired output\n",
        "\n",
        "#forward pass\n",
        "z = np.dot(weights, inputs) + bias\n",
        "output = sigmoid(z)\n",
        "\n",
        "#backward pass\n",
        "new_weights, new_bias = backward_pass(weights, bias, inputs, output, target)\n",
        "print(\"updated weights:\", new_weights)\n",
        "print(\"updated bias:\", new_bias)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx91xrB83FoK",
        "outputId": "9173db43-add2-4497-8c3e-76d5581c9723"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated weights: [0.80047441 0.39981023]\n",
            "Updated bias: 0.10094882975699739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backpropagation Algorithm\n",
        "The **backpropagation algorithm** is the systematic application of the backpropagation process over multiple iterations (epochs). During each epoch, it uses gradient descent to adjust the weights and biases to minimize the cost function.\n",
        "\n",
        "1. Initialize weights and biases randomly.\n",
        "2. For each training input, calculate the network output and cost.\n",
        "3. Compute gradients of the cost with respect to weights and biases, update them using stochastic gradient descent, and repeat until reaching desired accuracy."
      ],
      "metadata": {
        "id": "xK9OneKa3HVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training with backpropagation\n",
        "def train_network(inputs, targets, weights, bias, epochs=1000, lr=0.01):\n",
        "    for epoch in range(epochs):\n",
        "        for x, target in zip(inputs, targets):\n",
        "            #forward pass\n",
        "            z = np.dot(weights, x) + bias\n",
        "            output = sigmoid(z)\n",
        "\n",
        "            #compute gradients and update weights\n",
        "            error = output - target\n",
        "            d_output_d_z = output * (1 - output)\n",
        "            d_cost_d_z = error * d_output_d_z\n",
        "\n",
        "            weights -= lr * d_cost_d_z * x\n",
        "            bias -= lr * d_cost_d_z\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "targets = np.array([0, 0, 0, 1])\n",
        "\n",
        "#initial weights and bias\n",
        "weights = np.random.rand(2) #random intialization\n",
        "bias = 0.1\n",
        "\n",
        "#train\n",
        "trained_weights, trained_bias = train_network(inputs, targets, weights, bias)\n",
        "print(\"trained weights:\", trained_weights)\n",
        "print(\"trained bias:\", trained_bias)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYaS4vUI3IrF",
        "outputId": "dbb610d2-3cc8-4dec-95fb-be9f38b11c4f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained weights: [0.71819519 0.84419674]\n",
            "Trained bias: -1.4574899292589012\n"
          ]
        }
      ]
    }
  ]
}